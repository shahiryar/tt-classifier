{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 11:45:59.010663: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-02-15 11:45:59.010711: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-02-15 11:45:59.010724: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-02-15 11:45:59.010822: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-15 11:45:59.010893: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at shahiryar/tt_abstract_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"shahiryar/tt_abstract_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = classifier.tokenizer\n",
    "model = classifier.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERT’s maximum input length:\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess the log data\n",
    "import pandas as pd\n",
    "log_df = pd.read_csv('../dataset/log.csv')\n",
    "log_df = log_df[[\"abstract\", \"label\"]]\n",
    "log_df.columns = ['text', 'label']\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_v/dyxdn97j4931qv74z2fkldxr0000gn/T/ipykernel_948/3182036256.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.Title = df_clean[\"Title\"].map(to_lower)\n",
      "/var/folders/_v/dyxdn97j4931qv74z2fkldxr0000gn/T/ipykernel_948/3182036256.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.Abstract = df_clean.Abstract.map(to_lower)\n",
      "/var/folders/_v/dyxdn97j4931qv74z2fkldxr0000gn/T/ipykernel_948/3182036256.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.TT = df_clean.TT.map(strip)\n",
      "/var/folders/_v/dyxdn97j4931qv74z2fkldxr0000gn/T/ipykernel_948/3182036256.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.TC = df_clean.TC.map(strip)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the knowledge-based view, nested heterogeneity...</td>\n",
       "      <td>TT5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a multidimensional conceptualization of enviro...</td>\n",
       "      <td>TT5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>an attention-based view of real options reason...</td>\n",
       "      <td>TT5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the dark side of organizational paradoxes: the...</td>\n",
       "      <td>TT5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exploring intuition and its role in managerial...</td>\n",
       "      <td>TT5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  the knowledge-based view, nested heterogeneity...   TT5\n",
       "1  a multidimensional conceptualization of enviro...   TT5\n",
       "2  an attention-based view of real options reason...   TT5\n",
       "3  the dark side of organizational paradoxes: the...   TT5\n",
       "4  exploring intuition and its role in managerial...   TT5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collect previous dataset\n",
    "labeled_dataset_path = '../dataset/Paper Abstracts and Categorization Shahiryar.csv'\n",
    "labled_df = pd.read_csv(labeled_dataset_path)\n",
    "labled_df.head()\n",
    "\n",
    "df_clean = labled_df[[\"Paper ID\", \"Title\", \"Abstract\", \"TT\", \"TC\"]]\n",
    "to_lower = lambda s: s.lower() if s==s else \"\"\n",
    "strip = lambda s: s.strip() if s==s else \"\"\n",
    "df_clean.Title = df_clean[\"Title\"].map(to_lower)\n",
    "df_clean.Abstract = df_clean.Abstract.map(to_lower)\n",
    "df_clean.TT = df_clean.TT.map(strip)\n",
    "df_clean.TC = df_clean.TC.map(strip)\n",
    "\n",
    "df_clean = df_clean.iloc[:-3, :-1]\n",
    "df_clean['text'] = df_clean['Title']+\" \"+df_clean['Abstract']\n",
    "df_clean['label'] = df_clean['TT']\n",
    "df_clean = df_clean.iloc[:, -2:]\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_clean, log_df])\n",
    "df.drop_duplicates(subset=[\"text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(df.label.value_counts().index))\n",
    "NUM_CLASSES = len(classes)\n",
    "id2label = { i: el for i, el in enumerate(classes)}\n",
    "label2id = { el: i for i, el in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TT1': 0, 'TT5': 1}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_v/dyxdn97j4931qv74z2fkldxr0000gn/T/ipykernel_948/890478537.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.label = df.label.replace(label2id)\n"
     ]
    }
   ],
   "source": [
    "df.label = df.label.replace(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the knowledge-based view, nested heterogeneity...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a multidimensional conceptualization of enviro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>an attention-based view of real options reason...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the dark side of organizational paradoxes: the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exploring intuition and its role in managerial...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  the knowledge-based view, nested heterogeneity...      1\n",
       "1  a multidimensional conceptualization of enviro...      1\n",
       "2  an attention-based view of real options reason...      1\n",
       "3  the dark side of organizational paradoxes: the...      1\n",
       "4  exploring intuition and its role in managerial...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahiryarsaleem/Documents/Projects/TT Classifier/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 37\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 35.5kB/s]\n",
      "config.json: 100%|██████████| 483/483 [00:00<00:00, 4.72MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:01<00:00, 143kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.47MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERT’s maximum input length:\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 37/37 [00:00<00:00, 1356.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Apply the preprocessing function over the entire dataset, use 🤗 Datasets map function\n",
    "tokenized_papers = train_dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that passes your predictions and labels to compute to calculate the accuracy:\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "2024-02-17 09:01:22.911672: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-02-17 09:01:22.911700: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-02-17 09:01:22.911708: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-02-17 09:01:22.911767: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-17 09:01:22.911812: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 7\n",
    "num_epochs = 15\n",
    "batches_per_epoch = len(tokenized_papers) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  55%|█████▍    | 147M/268M [01:35<01:02, 1.93MB/s] Error while downloading from https://cdn-lfs.huggingface.co/distilbert-base-uncased/5e3f1108e3cb34ee048634875d8482665b65ac713291a7e32396fb18f6ff0063?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1708401325&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODQwMTMyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC81ZTNmMTEwOGUzY2IzNGVlMDQ4NjM0ODc1ZDg0ODI2NjViNjVhYzcxMzI5MWE3ZTMyMzk2ZmIxOGY2ZmYwMDYzP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=iiM0komK%7E943Are2%7E2bwN4GIr0cyOO57w7D83XquLN4CAMoyGi21FxEt-qOlj8FTW4ubUHau-Crwg9J6bKgx-u-INPpDb94Rz7U9unTmtmko1NBo-j7e%7E8FJ5RKz6X5%7E9oinerRQ78ZciyR9UeH09V2P8E42%7EP-k0sMjsvM%7EVutVsuP6cxpeRC5AqFxN7xWVx9YRA7Q5VaYlNRaNrpY2x%7Eaj9FfLzAMt9mzmb087xPtFDqLE1Q7FwNuWpD9sx7b-eFOyAiLQv3eYM4QTmlFjWdhMvLLIlcenVChXG9y99UbXU%7EQ3NE95kDRmXBEHqe4nlfDGyf7IuVtX6baQxFqfgQ__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs.huggingface.co/distilbert-base-uncased/5e3f1108e3cb34ee048634875d8482665b65ac713291a7e32396fb18f6ff0063?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1708401325&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODQwMTMyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC81ZTNmMTEwOGUzY2IzNGVlMDQ4NjM0ODc1ZDg0ODI2NjViNjVhYzcxMzI5MWE3ZTMyMzk2ZmIxOGY2ZmYwMDYzP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=iiM0komK%7E943Are2%7E2bwN4GIr0cyOO57w7D83XquLN4CAMoyGi21FxEt-qOlj8FTW4ubUHau-Crwg9J6bKgx-u-INPpDb94Rz7U9unTmtmko1NBo-j7e%7E8FJ5RKz6X5%7E9oinerRQ78ZciyR9UeH09V2P8E42%7EP-k0sMjsvM%7EVutVsuP6cxpeRC5AqFxN7xWVx9YRA7Q5VaYlNRaNrpY2x%7Eaj9FfLzAMt9mzmb087xPtFDqLE1Q7FwNuWpD9sx7b-eFOyAiLQv3eYM4QTmlFjWdhMvLLIlcenVChXG9y99UbXU%7EQ3NE95kDRmXBEHqe4nlfDGyf7IuVtX6baQxFqfgQ__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs.huggingface.co/distilbert-base-uncased/5e3f1108e3cb34ee048634875d8482665b65ac713291a7e32396fb18f6ff0063?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1708401325&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODQwMTMyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC81ZTNmMTEwOGUzY2IzNGVlMDQ4NjM0ODc1ZDg0ODI2NjViNjVhYzcxMzI5MWE3ZTMyMzk2ZmIxOGY2ZmYwMDYzP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=iiM0komK%7E943Are2%7E2bwN4GIr0cyOO57w7D83XquLN4CAMoyGi21FxEt-qOlj8FTW4ubUHau-Crwg9J6bKgx-u-INPpDb94Rz7U9unTmtmko1NBo-j7e%7E8FJ5RKz6X5%7E9oinerRQ78ZciyR9UeH09V2P8E42%7EP-k0sMjsvM%7EVutVsuP6cxpeRC5AqFxN7xWVx9YRA7Q5VaYlNRaNrpY2x%7Eaj9FfLzAMt9mzmb087xPtFDqLE1Q7FwNuWpD9sx7b-eFOyAiLQv3eYM4QTmlFjWdhMvLLIlcenVChXG9y99UbXU%7EQ3NE95kDRmXBEHqe4nlfDGyf7IuVtX6baQxFqfgQ__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs.huggingface.co/distilbert-base-uncased/5e3f1108e3cb34ee048634875d8482665b65ac713291a7e32396fb18f6ff0063?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1708401325&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODQwMTMyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC81ZTNmMTEwOGUzY2IzNGVlMDQ4NjM0ODc1ZDg0ODI2NjViNjVhYzcxMzI5MWE3ZTMyMzk2ZmIxOGY2ZmYwMDYzP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=iiM0komK%7E943Are2%7E2bwN4GIr0cyOO57w7D83XquLN4CAMoyGi21FxEt-qOlj8FTW4ubUHau-Crwg9J6bKgx-u-INPpDb94Rz7U9unTmtmko1NBo-j7e%7E8FJ5RKz6X5%7E9oinerRQ78ZciyR9UeH09V2P8E42%7EP-k0sMjsvM%7EVutVsuP6cxpeRC5AqFxN7xWVx9YRA7Q5VaYlNRaNrpY2x%7Eaj9FfLzAMt9mzmb087xPtFDqLE1Q7FwNuWpD9sx7b-eFOyAiLQv3eYM4QTmlFjWdhMvLLIlcenVChXG9y99UbXU%7EQ3NE95kDRmXBEHqe4nlfDGyf7IuVtX6baQxFqfgQ__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model.safetensors: 100%|██████████| 268M/268M [00:18<00:00, 1.45MB/s]\n",
      "model.safetensors:  90%|█████████ | 241M/268M [02:05<00:35, 752kB/s] \n",
      "model.safetensors:  55%|█████▍    | 147M/268M [04:09<03:26, 588kB/s] \n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=NUM_CLASSES, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_papers,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#==============================\n",
    "#Compile Text Classifier Model\n",
    "#==============================\n",
    "model.compile(optimizer=optimizer)  # No loss argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66955779 (255.42 MB)\n",
      "Trainable params: 66955779 (255.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import os\n",
    "ACCESS_TOKEN = os.environ['HUGGINGFACE_HUB_ACCESS_CODE']\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"tt_abstract_classifier\",\n",
    "    tokenizer=tokenizer,\n",
    "    hub_token=ACCESS_TOKEN\n",
    ")\n",
    "\n",
    "#metric_callback = KerasMetricCallback(metric_fn=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 09:10:41.916924: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-02-17 09:10:42.495920: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp_10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 45s 8s/step - loss: 1.0905\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.9642\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.8880\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.7704\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.6767\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 8s 942ms/step - loss: 0.6824\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 4s 757ms/step - loss: 0.6364\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 4s 891ms/step - loss: 0.5643\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 4s 839ms/step - loss: 0.4871\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.4736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2f3320bd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer([\"some new text test text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.argmax(pred.logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TT5'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[pred_labels[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store classes in the classes file\n",
    "import json\n",
    "with open('../resources/label2id.json', 'w') as classes_file:\n",
    "    json.dump(label2id, classes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
